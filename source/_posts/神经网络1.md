---
title: 神经网络学习(一) --- 卷积部分
date: 2020-04-22 17:07:18
tags: [神经网络，machine learning]
categories: [神经网络，machine learning]
---
烂笔头疗法，学习下神经网络的卷积部分，（只是理论部分，改天有时间实操几个问题。。
<!--more-->
一、卷积
1.1 卷积的概念
摆个图，应该就懂了（逃
![alt](/images/神经网络-1/figure1.jpg)
![alt](/images/神经网络-1/figure2.gif)
卷积的作用：在于能提供和挖掘物体的某些共有特征
先放一张图在下面，然后介绍一些专有名词：
![alt](/images/神经网络-1/figure3.png)
input_channel: 输入数据的channel个数，对应每个kernel(filter)有多少个channel。
output_channel:输出数据(feature map)的channel个数，对应有多少个kernel(filter)。
kernel(filter): 卷积核，如图所示，黄色的部分
padding: 在图片最外圈补零。如果padding等于，就相当于补两圈0，最后得到输入的size为(h+2,w+2)。
stride:卷积核每次移动的补偿，上图卷积核stride为1。
feature map:最后得到的输出。
如何计算feature map的size
$$outputsize=(inputsize-kernelsize+2 \times padding)/stride+1$$
1.2 特殊的卷积 ---- 1x1卷积的作用
1）channel-wise full connection （跨通道信息结合）
可以按照将feature map中的(x,y)的不同channel连接起来，这样相比全连接层参数更少，更节省空间。在16年CVPR 《Context Encoders: Feature Learning by Inpainting》有提及。如果想保持输入和输出size一样，使用全连接的参数比1x1卷积多。
2）升维、降维：这主要是指channel维度。
二、反卷积（转置卷积）
2.1 反卷积的概念
反卷积主要作用，举个我现在正在图像修补的任务来说，图像修补主要采用类似Encoder-Decoder的结构，Encoder主要采用卷积将语义信息编码，在decoder阶段主要使用反卷积（转置卷积）进行解码，还原原图的分辨率。我认为这种结构应该常见于类似UNet的网络的结构。（或是图像修补任务中）。
那么反卷积是什么呢？
下图是反卷积的常见的过程。
![alt](/images/神经网络-1/figure4.gif)
在这里,$ kernel_size = 3, padding = 1, stride = 2$。
那么你可能会觉得好奇，按照正常卷积那套，这里的stride不应该是1吗。**这里就是反卷积和卷积的不同。反卷积的stride并不代表每次点乘后需要将kernel移动多少个单位，而是代表我需要在input里面插入每两列、每两行插入stride-1的列（行）的0。注：padding决定边界插入多少列（行）的0。**
下图给个stride为1的反卷积，这个实际上和卷积的参数是相同。（$padding=2,stride=1,kernelsize=3$）
![alt](/images/神经网络-1/figure5.gif)
2.2 如何计算反卷积的output_size
反卷积的output_size其实可以先算出中间插入0后新的input_size，再利用stride=1、padding、kernel_size计算新的feature_map的大小。
知乎硬是没看懂。。。（那个取mod不是搞笑吗，不是变成蛋生鸡还是鸡生蛋的问题了吗？那这样我还不如直接模拟，或者就考虑没有余数的情况）
https://www.zhihu.com/question/48279880
反正正常情况：$ ((inputsize - 1) \times stride + 2 \times padding - kernelsize) / 1$,(化简过，别怀疑为什么没有+1)。
三、空洞卷积(膨胀卷积)
3.1 空洞卷积的概念
还是给个图感受下：
![alt](/images/神经网络-1/figure6.png)
dilate=2，kernel_size=3,padding=0。
3.2 感受野概念及应用
感受野就是当前的feature map的每一个节点和多少个输入节点相关。例如下图，倒数第二层的feature map每个节点由输入的两个节点构成。那么相当于感受野是2。我们上面（蓝色）的图的倒数第3层节点感受野是4，而在第二层跳过一个节点做连接的感受野是5，感受野增大，主要是跳过一些节点后，重复感受野区域减小了，非重复区域增多，所以导致感受野整体增大。
图像修补
![alt](/images/神经网络-1/figure7.jpg)
应用：感受野通常应用在目标检测、语义分割、图像修补等领域。例如基于anchor的目标检测，如果卷积捕捉的语义太小，那么很有可能不足以表征整个anchor的语义无法进行正确的目标检测。例如盲人摸大象，你只摸到象腿，能猜出这个是象吗？对于图像修补，如果捕捉语义太小，很有可能捕捉的仍是空洞的语义，这样必然会导致结果更加模糊。如下图所示：
![alt](/images/神经网络-1/figure8.png)
不过以上的推理都是理论的感受野，并不是在卷积过程中真正的感受野。由于在卷积过程中，事实上会有些节点重复编码到一个feature map的节点中，这样该节点对feature map的这个节点贡献较大。所以并不是感受野中每个节点对feature map的某一节点影响是等同的。一般是呈现高斯分布（在感受野中间的节点值对该feature map的这个节点影响较大）。
来源：2016 NIPS 《Understanding the Effective Receptive Field in Deep Convolutional Neural Networks》
一个本人觉得讲的不错解释：
https://zhuanlan.zhihu.com/p/40267131
3.3 空洞卷积与卷积的区别
普通卷积也可以增加感受野，通常由两种操作：
a.卷积后增加池化层
b.卷积加stride
a操作的缺点在于池化层增加参数和计算的复杂度，而且损失了更多的信息。
b操作的缺点在于加入stride后，感受野的增加是延迟的。不能在当前层起效。比如说你当前进行stride=2的卷积，那么想要增加感受野，必须在这之后再加一层卷积，这层卷积后得到feature map比原来经过stride=1卷积后在卷积的feature map感受野大。
而空洞卷积则不存在上述问题，即空洞能及时的进行感受野增大，能保证用较小的参数、损失较小的信息实现feature map的感受野的增加增加。
五、depthwise卷积、pointwise卷积
首先我们要知道常规卷积的操作是怎么样，如下图所示：
![alt](/images/神经网络-1/figure11.png)
而事实上，我们可以拆分成两步：Depthwise Convolution与Pointwise Convolution。
首先是Depthwise Convolution：
如下图所示，有多少个input_channel，就有多少个深度卷积核(channel都是1)，且每个深度卷积核只和对应的channel的输入进行卷积。
![alt](/images/神经网络-1/figure12.png)
而后是Pointwise Convolution:
![alt](/images/神经网络-1/figure13.png)
如下图所示，output_channel = filter_count，而每一个卷积核的size为1 x 1 x input_channel。
可以发现：原来参数个数为：$3 \times 3 \times 3 \times 4 = 108$
而现在的参数: $3 \times 3 \times 3 + 1 \times 1 \times 4 = 31 $
参数近乎减少到原来的1/3，让我们再来看卷积的计算次数（只关注乘法，因为乘法消耗的CPU or GPU时间周期更大）：
原来： $origincnt =  H \times W \times 4 \times 3 \times 3 $
而现在：$curcnt = H \times W \times 3 \times 3 \times 3 + H \times W \times 4 \times 1 \times 1 $
乘法次数大幅度减少。
这样能够大幅度减少参数和卷积次数，大大降低时间复杂度和空间复杂度。是一种优化的方式。
六、其他卷积
部分卷积
最早是由ECCV 2018年的一篇论文《Image Inpainting for Irregular Holes Using Partial Convolutions》。由NVIDIA提出来一种概念，他们团队认为普通卷积没有考虑每个节点对该区域影响权重，而错误将每个节点视为等同，这样容易导致填补过程的歧义（我口胡的：可以理解为动态规划的具有后效性，比如说当前有缺失像素A和缺失像素B，卷积核先利用缺失像素B的信息先将缺失像素A进行补洞，而后利用缺失像素A的信息再对缺失像素B进行补洞，这样容易混乱，如果我们加入权值，就可以令B像素对A像素填充权重更小以避免这钟情况的发生。），这因为这种特性，所以可以使用部分卷积有利于对非规则区域的像素进行填充。（因为如果直接卷积，容易导致补洞的过拟合，如把很多没用的区域考虑进去了，出现模糊的现象。）
具体方法如下：
![alt](/images/神经网络-1/figure9.png)
![alt](/images/神经网络-1/figure10.png)
门卷积
是在部分卷积上的改进版本。由Yu、Adobe Research、ByteDance Research的团队在ICCV(oral)《Free-Form Image Inpainting with Gated Convolution》提出来一项技术。Yu认为部分卷积并不能很准确区分那些像素是有效的，那些像素对该区域的语义信息编码、或者是补洞是有效的。例如说下面两个Patch，在部分卷积中则会视为等同：
$$
\begin {pmatrix}
     1 & 1 & 1 \\
     1 & 1 & 1\\
     1 & 1 & 1 
\end {pmatrix} 
$$
$$
\begin {pmatrix}
     0 & 0 & 0 \\
     0 & 0 & 1\\
     0 & 0 & 1 
\end {pmatrix} 
$$
其中0表示missing pixel，而1表示known pixel。很明显上面的区域更容易看成（编码）一个known region，对补洞的贡献更大。而下面的区域更容易被编码成unkown region，对补洞贡献要小一些，而在部分卷积中这两者是等同，明显不合理，当然作者还举了其他不合理的地方，这里不一一列出了。（大概在文中Introduction的第四段）
