---
title: 隐式马尔科夫的学习
date: 2020-04-29 20:46:04
tags: [machine learning]
categories: [machine learning]
---
学习一下隐式马尔科夫这个问题。（好像除了参数估计，就是个概率DP啊，摔
<!--more-->
一、基本概念
所有可能状态集合： $$ Q = (q_1, q_2, ... , q_n) $$
所有可观测集合：   $$ V = (v_1, v_2, ... , v_M) $$
长度为T的状态序列：$$ I = (i_1, i_2, ... , i_T) $$
长度为T的观测序列：$$ O = (o_1, o_2, ... , o_T) $$
初始概率分布：     $$ \pi = (\pi_{1}, \pi_{2}, ... , \pi_{n}) $$
其中：
$$ \pi_{i} = P(i_t=q_i) , i=1,2,...N $$
状态转移矩阵：     $$ A = [a_{ij}] $$
其中：
$$ a_{ij} = P(i_{t+1} = q_j | i_t = qi), i=1,2,...,N; j=1,2,...,N. $$
观测转移矩阵：     $$ B = [b_{ij}] $$
其中：
$$ b_{ij} = P(o_t = v_j | i_t = q_i), i=1,2...,N;j=1,2,...,M $$
两个基本假设：
1）齐次马尔科夫性假设：即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于前一个时刻的状态。与其他时刻状态及观测无关，也与t时刻无关。
2）观测独立假设：即假设任意时刻观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。
**具体例子，请参考《统计学习方法》 第10章隐式马尔科夫的第一节，对着例子思考，这些概念会很清晰。**
三个基本问题：
a. 概率计算，给定观测序列，计算在参数$$\lambda$$下的该观测序列的概率是多少？
b. 参数估计，给定观测序列，估计参数
c. 预测，给定观测序列，预测导致对应隐藏状态是多少。
二、概率计算问题
2.1 前向概率计算
定义foward[i][j]表示第j时刻，状态为i的概率是多少？
初值：$$ foward[i][1] = \pi[i] \times b[i][o_1] $$
转移方程： $$ foward[i][t] = \sum_{j=1}^N{(foward[j][t-1] \times a[j][i])} \times b[i][o_t] , t \gt 1 $$
最终结果： $$ ans = \sum_{i=1}^N forward[i][T] $$
2.2 后向概率计算
定义backward[i][j]表示从j+1时刻起，状态为i的概率是多少？
初值： $$ backward[i][T] = 1 $$
转移方程： $$ backward[i][t] = \sum_{j=1}^N{(backward[j][t+1] \times a[j][i])} \times b[j][o_{t+1}] , t \lt T $$
最终结果： $$ ans = \sum_{i=1}^N{(\pi_i \times b[i][o_1] \times backward[i][1])} $$
注：因为T+1以后的结果不再关心，所以可以虚拟出一个节点，将所有状态节点概率流入这个节点，所以这个节点的概率是1，结合后向概率的概念，所以backward[i][T]被初始化为1。
三、参数估计（简单介绍，暂时略过，在学习最大似然估计和最大后验估计等等参数估计方法，会补充说明）
主要是对转移状态矩阵和观测转移矩阵的估计，根据输入的训练数据的输入（是否包括状态序列），又分为有监督和无监督型学习方法。
四、预测问题
4.1 近似算法
这个算法很简单，是一种贪心算法，就是每个时刻都选择一个概率最大的状态作为该时刻的估计状态。
首先定义：
前向概率： $$ \alpha_{t}(i) = forward[i][t] $$
后向概率:  $$ \beta_{t}(i) = backward[i][t] $$
那么时刻t处于状态$$q_i$$的概率$$ \gamma_{t}(i) = \frac{\alpha_{t}(i) \times \beta_{t}(i)}{\sum_{j=1}^N{\alpha_{t}(j) \times \beta_{t}(j)}} $$
那么得到最有可能的状态
$$ I_{t} = arg\,\max_{1 \le i \le N} [\gamma_{t}(i)] $$
4.2 维比特算法
这个算法就是概率DP：
定义DP[i][t]表示t时刻下，状态为$$q_i$$的概率。
初值：   $$ DP[i][1] = \pi_{i} \times b[i][o_1] $$
转移:    $$ DP[i][t] = max(DP[j][t-1] \times b[j][o_{t-1}] \times a[j][t])，t \lt T $$
最终结果：$$ ans = max(DP[i][T]) $$
然后在回溯求得这个路径就行了。
用C++ 搞搞这个代码，python的版本太多了。。。
{% codeblock lang:C %}
#include <cstdio>
#include <cstring>
#include <cmath>
#include <iostream>
#define MAXN 100
class HMM
{
public:
	int N, M, T;   //N表示可能的状态数，M表示可能的观测数，T表示时间
	int O[MAXN];   //输入观测序列
	double forward[MAXN][MAXN], backward[MAXN][MAXN], pi[MAXN]; //forward表示前向概率，backward表示后向概率
	double A[MAXN][MAXN], B[MAXN][MAXN];   //A状态转移矩阵，B观测转移矩阵
	double DP[MAXN][MAXN];				//Predict中的中间结果
	int mark[MAXN][MAXN];
	int L[MAXN];						//Predict序列值
	HMM(){}
	void Input()
	{
		//输入状态转移矩阵
		for (int i = 1; i <= N; i++)
			for (int j = 1; j <= N; j++)
				std::cin >> A[i][j];
		//输入观测转移矩阵
		for (int i = 1; i <= N; i++)
			for (int j = 1; j <= M; j++)
				std::cin >> B[i][j];
		//输入初始概率
		for (int i = 1; i <= N; i++)
			std::cin >> pi[i];
		for (int i = 1; i <= T; i++)
			std::cin >> O[i];
	}
	HMM(int n,int m,int t)
	{
		N = n;
		M = m;
		T = t;
		Input();
	}
	void init_forward()
	{
		for (int i = 1; i <= N; i++)
			forward[i][1] = pi[i] * B[i][O[1]];
	}
	void init_backward()
	{
		for (int i = 1; i <= N; i++)
			backward[i][T] = 1;
	}
	void init_predict()
	{
		for (int i = 1; i <= N; i++)
			DP[i][1] = pi[i] * B[i][O[1]];
	}
	void solve()
	{
		for (int t = 2; t <= T; t++)
		{
			for (int i = 1; i <= N; i++)
			{
				mark[i][t] = 0;
				DP[i][t] = 0;
				for (int j = 1; j <= N; j++)
				{
					double tmp = DP[j][t-1] * A[j][i] * B[i][O[t]];
					if (DP[i][t] < tmp)
					{
						DP[i][t] = tmp;
						mark[i][t] = j;
					}
				}
			}
		}
		double ans = 0.0;
		for (int i = 1; i <= N; i++)
		{
			if (ans < DP[i][T])
			{
				ans = DP[i][T];
				mark[0][T + 1] = i;
			}
		}
	}
	double forward_cal()
	{
		init_forward();
		for (int t = 2; t <= T; t++)
		{
			for (int i = 1; i <= N; i++)
			{
				forward[i][t] = 0;
				for (int j = 1; j <= N; j++)
				{
					forward[i][t] += forward[j][t - 1] * A[j][i] * B[i][O[t]];  // j-->i，且i的状态取o[t]
				}
			}
		}
		double ans = 0.0;
		for (int i = 1; i <= N; i++)
			ans = ans + forward[i][T];
		return ans;
	}
	double backward_cal()
	{
		init_backward();
		for (int t = T - 1; t > 0; t--)
		{
			for (int i = 1; i <= N; i++)
			{
				backward[i][t] = 0;
				for (int j = 1; j <= N; j++)
				{
					backward[i][t] += backward[j][t + 1] * A[i][j] * B[j][O[t+1]];
				}
			}
		}
		double ans = 0.0;
		for (int i = 1; i <= N; i++)
			ans = ans + backward[i][1] * pi[i] * B[i][O[1]];
		return ans;
	}
	void get_list(int t,int pre)
	{
		L[t] = mark[pre][t + 1];
		if (t>1)
			get_list(t - 1, L[t]);
	}
	void output()
	{
		for (int i = 1; i <= T; i++)
			std::cout << L[i] << " ";
		std::cout << "\n";
	}
	void predict()
	{
		//初始化
		init_predict();
		solve();
		get_list(T, 0);
		output();
	}
};
int main()
{
	HMM work=HMM(3, 2, 3);
	std::cout << work.forward_cal() << "\n";
	std::cout << work.backward_cal() << "\n";
	work.predict();
	system("pause");
	return 0;
}
/*
0.5 0.2 0.3
0.3 0.5 0.2
0.2 0.3 0.5

0.5 0.5
0.4 0.6
0.7 0.3

0.2 0.4 0.4

1 2 1
*/
{% endcodeblock %}
书中例子：
{% codeblock lang:C %}
Input:
0.5 0.2 0.3
0.3 0.5 0.2
0.2 0.3 0.5

0.5 0.5
0.4 0.6
0.7 0.3

0.2 0.4 0.4

1 2 1

Output:
0.130218
0.130218
3 3 3
{% endcodeblock %}